{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "parent_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_folder)\n",
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import scipy.sparse as sp\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "from anndata import AnnData\n",
    "from matplotlib import pyplot as plt\n",
    "from py_pcha import PCHA\n",
    "from scipy.sparse import issparse\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sympy.physics.units import current\n",
    "from tqdm import tqdm\n",
    "from kneed import KneeLocator\n",
    "import torch\n",
    "import scvi\n",
    "import numpy as np\n",
    "from scvi.train import TrainingPlan\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import bar_nick_utils\n",
    "import covet_utils\n",
    "\n",
    "importlib.reload(bar_nick_utils)\n",
    "importlib.reload(covet_utils)\n",
    "from covet_utils import compute_covet\n",
    "\n",
    "from bar_nick_utils import preprocess_rna, preprocess_protein, plot_archetypes, \\\n",
    "    get_cell_representations_as_archetypes_cvxpy, reorder_rows_to_maximize_diagonal, evaluate_distance_metrics, \\\n",
    "    plot_archetypes_matching, compare_matchings, find_best_pair_by_row_matching, add_spatial_data_to_prot, \\\n",
    "    clean_uns_for_h5ad, get_latest_file,get_latest_file,plot_latent_single\n",
    "\n",
    "plot_flag = True\n",
    "# computationally figure out which ones are best\n",
    "np.random.seed(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### reading in data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefixes = ['preprocessed_adata_rna_', 'preprocessed_adata_prot_']\n",
    "folder = 'data/'\n",
    "\n",
    "latest_files = {prefix: get_latest_file(folder, prefix) for prefix in file_prefixes}\n",
    "adata_1_rna = sc.read(latest_files['preprocessed_adata_rna_'])\n",
    "adata_2_prot = sc.read(latest_files['preprocessed_adata_prot_'])\n",
    "\n",
    "num_rna_cells = 6000\n",
    "num_protein_cells = 200000\n",
    "# num_rna_cells = num_protein_cells= 2000\n",
    "subsample_n_obs_rna = min(adata_1_rna.shape[0], num_rna_cells)\n",
    "subsample_n_obs_protein = min(adata_2_prot.shape[0], num_protein_cells)\n",
    "sc.pp.subsample(adata_1_rna, n_obs=subsample_n_obs_rna)\n",
    "sc.pp.subsample(adata_2_prot, n_obs=subsample_n_obs_protein)\n",
    "\n",
    "\n",
    "original_protein_num = adata_2_prot.X.shape[1]\n",
    "sc.pp.neighbors(adata_2_prot, use_rep='spatial_location', key_added='spatial_neighbors', n_neighbors=15)\n",
    "\n",
    "distances = adata_2_prot.obsp['spatial_neighbors_distances'].data\n",
    "log_transformed_distances = (distances + 1)  # since the distances are not normally distributed, log-transform them\n",
    "\n",
    "sns.histplot(log_transformed_distances[log_transformed_distances > 0])\n",
    "plt.title('Distribution of distances between spatial neighbors before applying cutoff')\n",
    "plt.show()\n",
    "distances_mean = log_transformed_distances.mean()\n",
    "distances_std = log_transformed_distances.std()\n",
    "\n",
    "one_std_dev = distances_mean + distances_std\n",
    "# Zero out neighbors in the connectivity matrix and remove them from the distances matrix if their distance is more than 2 standard deviations above the mean\n",
    "two_std_dev = distances_mean + 2 * distances_std\n",
    "\n",
    "# Get the indices of the neighbors to be zeroed out\n",
    "indices_to_zero_out = np.where(adata_2_prot.obsp['spatial_neighbors_distances'].data > two_std_dev)[0]\n",
    "\n",
    "# Zero out the corresponding entries in the connectivity matrix\n",
    "adata_2_prot.obsp['spatial_neighbors_connectivities'].data[indices_to_zero_out] = 0\n",
    "adata_2_prot.obsp['spatial_neighbors_distances'].data[indices_to_zero_out] = 0\n",
    "# Recompute the connectivity matrix\n",
    "non_zero_distances = adata_2_prot.obsp['spatial_neighbors_distances'].data[\n",
    "    adata_2_prot.obsp['spatial_neighbors_distances'].data > 0\n",
    "]\n",
    "sns.histplot(non_zero_distances)\n",
    "plt.title('Distribution of distances between spatial neighbors after applying cutoff')\n",
    "plt.show()\n",
    "# todo  I am using the data as if its notmarl and its not, so I need to see how to remove the tail in a different way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_2_prot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_adata_2_prot= adata_2_prot.copy()\n",
    "exp_adata_2_prot.X = np.exp(adata_2_prot.X)\n",
    "adata_2_prot.obsm['COVET'], adata_2_prot.obsm['COVET_SQRT'], adata_2_prot.uns['CovGenes'] = compute_covet(exp_adata_2_prot, k=21, spatial_key='spatial')\n",
    "adata_2_prot.obsm['COVET_SQRT'] = adata_2_prot.obsm['COVET_SQRT'].reshape(adata_2_prot.obsm['COVET_SQRT'].shape[0], -1)\n",
    "conv_sqrt = AnnData(adata_2_prot.obsm['COVET_SQRT'])\n",
    "conv_sqrt.obs = adata_2_prot.obs\n",
    "sc.pp.pca(conv_sqrt, n_comps=10)\n",
    "sc.pp.neighbors(conv_sqrt, n_neighbors=15, use_rep='X_pca', key_added='pca_neighbors')\n",
    "sc.tl.leiden(conv_sqrt, resolution=0.15, key_added='CN', neighbors_key='pca_neighbors')\n",
    "sc.tl.umap(conv_sqrt,neighbors_key='pca_neighbors')\n",
    "\n",
    "adata_2_prot.obs['CN'] = conv_sqrt.obs['CN']\n",
    "\n",
    "sc.tl.pca(adata_2_prot, n_comps=10)\n",
    "sc.pp.neighbors(adata_2_prot, n_neighbors=15, use_rep='X_pca', key_added='pca_neighbors')\n",
    "sc.tl.umap(adata_2_prot, min_dist=0.5, spread=1, neighbors_key='pca_neighbors')\n",
    "\n",
    "\n",
    "sc.pl.umap(conv_sqrt, color='CN',title='COVET_SQRT - CN as color')\n",
    "sc.pl.umap(adata_2_prot, color=['cell_type', 'CN'])\n",
    "sc.pl.scatter(adata_2_prot, x='X', y='Y', color=['CN', 'cell_type'],title=['spatial and CN as color', 'spatial and cell type as color'])\n",
    "# del conv_sqrt\n",
    "\n",
    "\n",
    "sil_score = silhouette_score(adata_2_prot.obsm['X_pca'], adata_2_prot.obs['CN'].astype(int))\n",
    "print(f\"Silhouette Score: {sil_score}\")\n",
    "\n",
    "assert sil_score < 0.5, \"Silhouette score is too high the CN labels should not depend on the spatial location\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the combined data\n",
    "sc.pp.normalize_total(conv_sqrt)\n",
    "conv_sqrt.X = conv_sqrt.X - conv_sqrt.X.min()\n",
    "\n",
    "sc.pp.pca(conv_sqrt, n_comps=50)\n",
    "\n",
    "# Compute the neighborhood graph using PCA representation\n",
    "sc.pp.neighbors(conv_sqrt, use_rep='X_pca')\n",
    "\n",
    "# Run UMAP for visualization\n",
    "sc.tl.umap(conv_sqrt)\n",
    "sc.pl.pca(conv_sqrt, color=['cell_types'], title=['PCA - Cell Types', 'PCA - Original Protein Clusters'])\n",
    "\n",
    "# Plot the UMAP embedding\n",
    "sc.pl.umap(conv_sqrt, color=['cell_types'], title=['UMAP - Cell Types', 'UMAP - Original Protein Clusters'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sil_score < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_data = zscore(neighbor_means, axis=0) # each cell we get the mean of the features of its neighbors\n",
    "# inertias = []\n",
    "# silhouette_scores = []\n",
    "# min_cn_lbls = 4\n",
    "# k_range = range(min_cn_lbls, 11)  # You can adjust this range as needed\n",
    "# for k in k_range:\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "#     kmeans.fit(normalized_data)\n",
    "#     inertias.append(kmeans.inertia_)sns.heatmap(adata_2_prot.obsp kmeans.labels_)\n",
    "#     silhouette_scores.append(score)\n",
    "# # num_clusters = min_cn_lbls + np.diff(inertias, 2).argmin() \n",
    "# num_clusters = k_range[np.argmax(silhouette_scores)] \n",
    "# inertias =np.array(inertias)\n",
    "# silhouette_scores = np.array(silhouette_scores)\n",
    "# # norm in the range 0-1\n",
    "# silhouette_scores = (silhouette_scores - silhouette_scores.min()) / (silhouette_scores.max() - silhouette_scores.min())\n",
    "# inertias = (inertias - inertias.min()) / (inertias.max() - inertias.min())\n",
    "# # Plot the elbow curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(k_range,inertias, 'bo-', label='norm Inertia')\n",
    "# plt.plot(k_range, silhouette_scores, 'bo-', color='red', label='norm Silhouette Score')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Number of Clusters (k)')\n",
    "# plt.xticks(k_range)\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method for Optimal k')\n",
    "# plt.grid(True)\n",
    "# plt.axvline(x=num_clusters, color='red', linestyle='dashed')\n",
    "# plt.show()\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(normalized_data)\n",
    "# final_labels = np.array(kmeans.labels_)\n",
    "# if 'CN' in adata_2_prot.obs:\n",
    "#     adata_2_prot.obs.drop(columns=['CN'], inplace=True, errors='ignore')\n",
    "#     # adata_1_rna.obs.drop(columns=['CN'], inplace=True, errors='ignore')\n",
    "# adata_2_prot.obs['CN'] = pd.Categorical(final_labels)\n",
    "# # adata_1_rna.obs['CN'] = pd.Categorical(final_labels)\n",
    "# num_clusters = len(adata_2_prot.obs['CN'].unique())\n",
    "# palette = sns.color_palette(\"tab10\", num_clusters)  # \"tab10\" is a good color map, you can choose others too\n",
    "# adata_2_prot.uns['spatial_clusters_colors'] = palette.as_hex()  # Save as hex color codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(adata_2_prot.obsm[\n",
    "                    'COVET_SQRT'])  # less noisy because we mean, makes sense as we made sure each cell is of same cell type\n",
    "    plt.title('convet sqrt')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(adata_2_prot.X.todense() if issparse(adata_2_prot.X) else adata_2_prot.X)\n",
    "    plt.title('Proteins expressions of each cell')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (different adata technique)\n",
    "if plot_flag:\n",
    "    # merge the CN to the protein data\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sc.pl.scatter(\n",
    "        adata_2_prot,\n",
    "        x='X', y='Y',\n",
    "        color='CN',  # got this CN from kmeans\n",
    "        title='spatial location with CN as color',\n",
    "        ax=ax,  # Use the ax created above\n",
    "        show=False  # Prevent scanpy from showing the plot immediately\n",
    "    )\n",
    "    # for x in horizontal_splits[1:-1]:  # Exclude the edges to avoid border doubling\n",
    "    #     ax.axvline(x=x, color='black', linestyle='--')\n",
    "    # for y in vertical_splits[1:-1]:  # Exclude the edges to avoid border doubling\n",
    "    #     ax.axhline(y=y, color='black', linestyle='--')\n",
    "    # plt.show()\\\n",
    "    # Run PCA on adata_2_prot.obsm['COVET_SQRT'] and keep 0.8 variance\n",
    "covet_adata = anndata.AnnData(adata_2_prot.obsm['COVET_SQRT'])\n",
    "\n",
    "sc.pp.pca(covet_adata, n_comps=50)\n",
    "cumulative_variance_ratio = np.cumsum(covet_adata.uns['pca']['variance_ratio'])\n",
    "n_comps_thresh = np.argmax(cumulative_variance_ratio >= 0.5) + 1\n",
    "covet_adata = AnnData(covet_adata.obsm['X_pca'][:, :n_comps_thresh])\n",
    "# covet_adata.obsm['X_pca'] = covet_adata.obsm['X_pca'][:, :n_comps_thresh]\n",
    "\n",
    "print(f\"Number of components explaining 0.5 variance: {n_comps_thresh}\")\n",
    "\n",
    "if plot_flag:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(covet_adata.X)  # less noisy because we mean, makes sense as we made sure each cell is of same cell type\n",
    "    plt.title('convet sqrt')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(adata_2_prot.X.todense() if issparse(adata_2_prot.X) else adata_2_prot.X)\n",
    "    plt.title('Proteins expressions of each cell')\n",
    "    plt.show()\n",
    "\n",
    "# rna no longer has the CN, using the protein\n",
    "covet_adata.obs['CN'] = pd.Categorical(adata_2_prot.obs['CN'])\n",
    "sc.pp.pca(covet_adata)\n",
    "sc.pp.neighbors(covet_adata)\n",
    "sc.tl.umap(covet_adata)\n",
    "sc.pl.umap(covet_adata, color='CN', title='UMAP of CN embedding')\n",
    "#  pop obsm from both\n",
    "# adata_2_prot.obsm.pop('COVET')\n",
    "# adata_2_prot.obsm.pop('COVET_SQRT')\n",
    "# covet_adata.obsm.pop('X_pca')\n",
    "# making sure the CN and the protein are distinct\n",
    "# adata_prot_cn_concat = anndata.concat([adata_2_prot, covet_adata], join='outer', label='modality',\n",
    "#                                       keys=['Protein', 'CN'])\n",
    "# X = adata_prot_cn_concat.X.toarray() if issparse(adata_prot_cn_concat.X) else adata_prot_cn_concat.X\n",
    "# X = np.nan_to_num(X)\n",
    "# adata_prot_cn_concat.X = X\n",
    "# sc.pp.pca(adata_prot_cn_concat)\n",
    "# sc.pp.neighbors(adata_prot_cn_concat)\n",
    "# if plot_flag:\n",
    "#     sc.tl.umap(adata_prot_cn_concat)\n",
    "#     sc.pl.umap(adata_prot_cn_concat, color=['CN', 'modality'],\n",
    "#                title=['UMAP of CN embedding to make sure they are not mixed',\n",
    "#                       'UMAP of CN embedding to make sure they are not mixed'])\n",
    "#     sc.pl.pca(adata_prot_cn_concat, color=['CN', 'modality'],\n",
    "#               title=['PCA of CN embedding to make sure they are not mixed',\n",
    "#                      'PCA of CN embedding to make sure they are not mixed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Compute neighborhood graph\n",
    "sc.pp.neighbors(adata_2_prot, n_neighbors=15, n_pcs=40)\n",
    "\n",
    "# Run Leiden clustering\n",
    "sc.tl.leiden(adata_2_prot, resolution=0.5, key_added='orig_prot_clusters')\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = adata_2_prot.obs['orig_prot_clusters'].cat.codes.values\n",
    "# Create a DataFrame for plotting\n",
    "adata_2_prot.obs['orig_prot_clusters'] = pd.Categorical(cluster_labels)\n",
    "sc.tl.umap(adata_2_prot)\n",
    "sc.pl.umap(adata_2_prot, color='orig_prot_clusters', title='Leiden clustering of protein data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PCA(n_components=100).fit(adata_2_prot.obsm['COVET_SQRT'])\n",
    "# pring variance explained\n",
    "print(np.cumsum(a.explained_variance_ratio_))\n",
    "# sns.heatmap(adata_2_prot.obsm['COVET_SQRT'])\n",
    "# norm cols\n",
    "adata_2_prot.obsm['COVET_SQRT'] = zscore(adata_2_prot.obsm['COVET_SQRT'], axis=0)\n",
    "sns.heatmap(adata_2_prot.obsm['COVET_SQRT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClusterPreservingTrainingPlan(TrainingPlan):\n",
    "    def __init__(self, module, **kwargs):\n",
    "        self.protein_dim = kwargs.pop('protein_dim', 30)\n",
    "        self.protein_vae = kwargs.pop('protein_vae', None)\n",
    "        self.cluster_weight = kwargs.pop('cluster_weight', 1.0)\n",
    "        self.initial_cluster_weight = self.cluster_weight\n",
    "        self.cluster_labels = kwargs.pop('cluster_labels', None)\n",
    "        self.batch_size = kwargs.pop('batch_size', 1000)\n",
    "        self.plot_x_times = kwargs.pop('plot_x_times', 10)\n",
    "        self.combined_adata = kwargs.pop('combined_adata', None)\n",
    "        \n",
    "        # Parameters for silhouette-based weight adjustment\n",
    "        self.target_silhouette = kwargs.pop('target_silhouette', 0.5)\n",
    "        self.silhouette_tolerance = kwargs.pop('silhouette_tolerance', 0.1)\n",
    "        self.min_cluster_weight = kwargs.pop('min_cluster_weight', 0.1)\n",
    "        self.max_cluster_weight = kwargs.pop('max_cluster_weight', 10.0)\n",
    "        \n",
    "        super().__init__(module, **kwargs)\n",
    "        \n",
    "        # Initialize tracking variables for silhouette calculation\n",
    "        self.initial_silhouette = None\n",
    "        self.all_latent = None\n",
    "        self.all_labels = None\n",
    "        self.last_epoch_checked = -1\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.protein_vae.module.parameters(),\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "        d = { # maybe add this?\n",
    "        \"optimizer\": optimizer,\n",
    "        \"gradient_clip_val\": 1.0,  # Critical for stability\n",
    "        \"gradient_clip_algorithm\": \"value\"\n",
    "        }\n",
    "        return d\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Initialize steps if needed\n",
    "        if not hasattr(self, 'total_steps'):\n",
    "            n_samples = len(self.combined_adata)\n",
    "            steps_per_epoch = int(np.ceil(n_samples / self.batch_size))\n",
    "            self.total_steps = steps_per_epoch * self.trainer.max_epochs\n",
    "            self.plot_interval = max(1, (steps_per_epoch * self.trainer.max_epochs) // self.plot_x_times)\n",
    "        self.loss_kwargs['kl_weight'] = 0.01\n",
    "        # Standard ELBO loss calculation\n",
    "        _, _, loss_output = self.module(batch, loss_kwargs=self.loss_kwargs)\n",
    "\n",
    "        elbo_loss = loss_output.loss\n",
    "        \n",
    "        # Get latent representation\n",
    "        x = batch[\"X\"]\n",
    "        inference_outputs = self.module.inference(\n",
    "            x, batch_index=batch[\"batch\"], n_samples=1\n",
    "        )\n",
    "        z = inference_outputs[\"qz\"].loc\n",
    "        \n",
    "        # Calculate cluster preservation loss\n",
    "        self.cluster_labels = self.cluster_labels.to(z.device)\n",
    "        cluster_loss = triplet_cluster_loss(z, self.cluster_labels[batch[\"labels\"]])\n",
    "\n",
    "        # Adjust cluster weight based on silhouette score (periodically)\n",
    "        if self.trainer.current_epoch >= 5 and batch_idx % 2 == 0:\n",
    "            # Calculate silhouette score on current batch only\n",
    "            current_silhouette = self.calculate_batch_silhouette(z, batch[\"labels\"])\n",
    "            self.log(\"batch_silhouette\", current_silhouette)\n",
    "            \n",
    "            # Adjust weight based on comparison with target\n",
    "            if current_silhouette < self.target_silhouette - self.silhouette_tolerance:\n",
    "                # Silhouette below target - increase weight (more gradually than before)\n",
    "                new_weight = min(self.cluster_weight * 1.2, self.max_cluster_weight)\n",
    "                self.cluster_weight = new_weight\n",
    "                \n",
    "            elif current_silhouette > self.target_silhouette + self.silhouette_tolerance:\n",
    "                # Silhouette above target - decrease weight\n",
    "                new_weight = max(self.cluster_weight / 1.2, self.min_cluster_weight)\n",
    "                self.cluster_weight = new_weight\n",
    "            \n",
    "            self.log(\"cluster_weight\", self.cluster_weight)\n",
    "\n",
    "        # Use the dynamic cluster weight\n",
    "        cluster_loss = self.cluster_weight * cluster_loss\n",
    "        dynamic_range = torch.max(z,dim=0)[0] -torch.min(z,dim=0)[0]\n",
    "        if torch.mean(dynamic_range) < 0.5:\n",
    "            dynamic_range_loss = -10000 *torch.log(torch.mean(dynamic_range))  # Encourage larger dynamic range\n",
    "        else:\n",
    "            dynamic_range_loss = 0\n",
    "        total_loss = elbo_loss + cluster_loss +dynamic_range_loss\n",
    "        \n",
    "        # Log losses\n",
    "        self.log(\"train_elbo_loss\", elbo_loss, on_epoch=False, on_step=True)\n",
    "        self.log(\"train_cluster_loss\", cluster_loss, on_epoch=False, on_step=True)\n",
    "        self.log(\"train_total_loss\", total_loss, on_epoch=False, on_step=True)\n",
    "        self.log(\"cluster_weight\", self.cluster_weight, on_epoch=False, on_step=True)\n",
    "        self.log(\"dynamic_range_loss\", dynamic_range_loss, on_epoch=False, on_step=True)\n",
    "        \n",
    "        # Plot periodically\n",
    "        if self.global_step > -1 and self.global_step % self.plot_interval == 0:\n",
    "            # print all losses vals:\n",
    "            print(f\"total_loss: {total_loss}, elbo_loss: {elbo_loss}, \\ncluster_loss: {cluster_loss}, dynamic_range_loss: {dynamic_range_loss}\")\n",
    "            print(f'dynamic_range: {torch.mean(dynamic_range)}')\n",
    "            plot_latent_single(z, self.combined_adata, batch[\"labels\"], \n",
    "                            color_label='orig_prot_clusters', \n",
    "                            title=f\"precentage {int(100*(self.global_step/self.total_steps))}\")\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # This runs exactly once per epoch after all validation batches\n",
    "        # Concatenate all stored latent representations\n",
    "        if self.trainer.current_epoch % max(1, self.trainer.max_epochs // self.plot_x_times) == 0:\n",
    "\n",
    "            if hasattr(self, 'validation_z') and len(self.validation_z) > 0:\n",
    "                all_z = torch.cat(self.validation_z, dim=0)\n",
    "                all_labels = torch.cat(self.validation_labels, dim=0)\n",
    "                \n",
    "                # Plot\n",
    "                plot_latent_single(all_z, self.combined_adata, all_labels, \n",
    "                                color_label='orig_prot_clusters',\n",
    "                                title=f\"_val_epoch_{self.trainer.current_epoch}\")\n",
    "                \n",
    "                # Clear stored data\n",
    "                self.validation_z = []\n",
    "                self.validation_labels = []\n",
    "\n",
    "    def calculate_batch_silhouette(self, z, batch_labels):\n",
    "        \"\"\"Calculate silhouette score on the current batch\"\"\"\n",
    "        \n",
    "        # Get latent representation and labels\n",
    "        latent = z.detach().cpu().numpy()\n",
    "        labels = self.cluster_labels[batch_labels].cpu().numpy()\n",
    "        \n",
    "        # Need at least 2 clusters and enough samples per cluster\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        if len(unique_labels) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Check if we have at least 2 samples per cluster\n",
    "        valid_clusters = True\n",
    "        for label in unique_labels:\n",
    "            if np.sum(labels == label) < 2:\n",
    "                valid_clusters = False\n",
    "                break\n",
    "        \n",
    "        if not valid_clusters:\n",
    "            return 0\n",
    "            \n",
    "        try:\n",
    "            # norm between 0 and 1 to avoid the latent space from just using large distances instead of distinct clusters\n",
    "\n",
    "            latent = (latent - latent.min(axis=0)) / (latent.max(axis=0) - latent.min(axis=0) + 1e-8)\n",
    "            score = silhouette_score(latent, labels.ravel())\n",
    "            # Add the size of the dynamic range as a loss\n",
    "            \n",
    "            return score \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating batch silhouette: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Standard validation logic (calculate losses, etc.)\n",
    "        _, _, loss_output = self.module(batch, loss_kwargs=self.loss_kwargs)\n",
    "        elbo_loss = loss_output.loss\n",
    "        \n",
    "        # Get latent representation\n",
    "        x = batch[\"X\"]\n",
    "        inference_outputs = self.module.inference(\n",
    "            x, batch_index=batch[\"batch\"], n_samples=1\n",
    "        )\n",
    "        z = inference_outputs[\"qz\"].loc\n",
    "        \n",
    "        # Calculate cluster loss\n",
    "        self.cluster_labels = self.cluster_labels.to(z.device)\n",
    "        cluster_loss = improved_triplet_cluster_loss(z, self.cluster_labels[batch[\"labels\"]])        \n",
    "\n",
    "\n",
    "        total_loss = elbo_loss + self.cluster_weight * cluster_loss\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_elbo_loss\", elbo_loss, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_cluster_loss\", cluster_loss, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_total_loss\", total_loss, on_epoch=True, on_step=False)\n",
    "        \n",
    "        # Store the latent representation for later use in on_validation_epoch_end\n",
    "        if not hasattr(self, 'validation_z'):\n",
    "            self.validation_z = []\n",
    "            self.validation_labels = []\n",
    "        \n",
    "        self.validation_z.append(z.detach().cpu())\n",
    "        self.validation_labels.append(batch[\"labels\"].detach().cpu())\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def plot_latent_space(self, z, labels, cell_types, step):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # TSNE for visualization\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        z_tsne = tsne.fit_transform(z.detach().cpu().numpy())\n",
    "        \n",
    "        # Plot cluster labels\n",
    "        plt.subplot(121)\n",
    "        scatter = plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=labels.cpu().numpy(), cmap='tab20', s=5)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'Latent Space - Clusters (Step {step})')\n",
    "        \n",
    "        # Plot cell types\n",
    "        plt.subplot(122)\n",
    "        scatter = plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=cell_types, cmap='tab20', s=5)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'Latent Space - Cell Types (Step {step})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'latent_space_step_{step}.png')\n",
    "        plt.close()\n",
    "def improved_triplet_cluster_loss(z, labels, margin=1.0, var_weight=0.1, use_semi_hard=True):\n",
    "    \"\"\"\n",
    "    Improved triplet loss with semi-hard negative mining and variance regularization\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    pairwise_dist = torch.cdist(z, z, p=2)\n",
    "    labels = labels.squeeze()\n",
    "    # Create same-cluster and different-cluster masks\n",
    "    labels_expanded_i = labels.unsqueeze(1)\n",
    "    labels_expanded_j = labels.unsqueeze(0)\n",
    "    same_cluster_mask = (labels_expanded_i == labels_expanded_j)\n",
    "    identity_mask = torch.eye(z.size(0), device=z.device, dtype=torch.bool)\n",
    "    \n",
    "    # Find valid positives (same cluster, not self)\n",
    "    positive_mask = same_cluster_mask & ~identity_mask\n",
    "    \n",
    "    # Find valid negatives (different cluster)\n",
    "    negative_mask = ~same_cluster_mask\n",
    "    \n",
    "    # Compute mean positive distance (average distance to same cluster)\n",
    "    mean_pos_dist = torch.sum(pairwise_dist * positive_mask.float(), dim=1) / (positive_mask.sum(dim=1) + 1e-8)\n",
    "    \n",
    "    # Calculate loss with semi-hard negative mining or standard hard mining\n",
    "    triplet_losses = torch.zeros_like(mean_pos_dist)\n",
    "    valid_anchors = torch.zeros_like(mean_pos_dist, dtype=torch.bool)\n",
    "    \n",
    "    for i in range(z.size(0)):\n",
    "        pos_dist = mean_pos_dist[i]\n",
    "        \n",
    "        # Skip if no positives for this anchor\n",
    "        if positive_mask[i].sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        neg_dists = pairwise_dist[i][negative_mask[i]]\n",
    "        \n",
    "        # Skip if no negatives for this anchor\n",
    "        if neg_dists.shape[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        valid_anchors[i] = True\n",
    "        \n",
    "        if use_semi_hard:\n",
    "            # Semi-hard negative: negatives that are further than the positive\n",
    "            # but still within margin (pos_dist < neg_dist < pos_dist + margin)\n",
    "            semi_hard_mask = (neg_dists > pos_dist) & (neg_dists < pos_dist + margin)\n",
    "            \n",
    "            if semi_hard_mask.any():\n",
    "                # Use closest semi-hard negative\n",
    "                semi_hard_negs = neg_dists[semi_hard_mask]\n",
    "                closest_semi_hard = semi_hard_negs.min()\n",
    "                triplet_losses[i] = torch.clamp(pos_dist - closest_semi_hard + margin, min=0.0)\n",
    "            else:\n",
    "                # If no semi-hard negatives, use closest negative\n",
    "                triplet_losses[i] = torch.clamp(pos_dist - neg_dists.min() + margin, min=0.0)\n",
    "        else:\n",
    "            # Hard negative mining (original approach)\n",
    "            triplet_losses[i] = torch.clamp(pos_dist - neg_dists.min() + margin, min=0.0)\n",
    "    \n",
    "    # Add variance regularization to encourage spread\n",
    "    # Calculate variance of each dimension of z\n",
    "    z_var = torch.var(z, dim=0).mean()\n",
    "    # We want to maximize variance (spread), so we minimize negative variance\n",
    "    var_loss = -var_weight * z_var\n",
    "    \n",
    "    # Only use valid triplets in final loss\n",
    "    if valid_anchors.any():\n",
    "        triplet_loss = triplet_losses[valid_anchors].mean()\n",
    "        return triplet_loss + var_loss\n",
    "    else:\n",
    "        return var_loss  # At least encourage spread if no valid triplets\n",
    "\n",
    "def triplet_cluster_loss(z, labels, margin=1.0):\n",
    "    \"\"\"\n",
    "    Triplet loss for cluster preservation that works with backpropagation\n",
    "    \"\"\"\n",
    "    labels = labels.squeeze()\n",
    "    # Compute pairwise distances\n",
    "    pairwise_dist = torch.cdist(z, z, p=2)\n",
    "    \n",
    "    # Create same-cluster mask (should be shape [batch_size, batch_size])\n",
    "    labels_expanded_i = labels.unsqueeze(1)  # [batch_size, 1]\n",
    "    labels_expanded_j = labels.unsqueeze(0)  # [1, batch_size]\n",
    "    same_cluster_mask = (labels_expanded_i == labels_expanded_j)  # [batch_size, batch_size]\n",
    "    \n",
    "    # Create identity mask to exclude self-pairs\n",
    "    batch_size = z.size(0)\n",
    "    identity_mask = torch.eye(batch_size, device=z.device, dtype=torch.bool)\n",
    "    \n",
    "    # Find valid positives (same cluster, not self)\n",
    "    positive_mask = same_cluster_mask & ~identity_mask\n",
    "    \n",
    "    # Find valid negatives (different cluster)\n",
    "    negative_mask = ~same_cluster_mask\n",
    "    \n",
    "    # Compute valid triplets mask - anchors that have at least one positive and one negative\n",
    "    valid_triplets = (positive_mask.sum(dim=1) > 0) & (negative_mask.sum(dim=1) > 0)\n",
    "    \n",
    "    if not valid_triplets.any():\n",
    "        return torch.tensor(0.0, device=z.device)\n",
    "    \n",
    "    # Set masked values for efficient max/min calculations\n",
    "    # For positives: replace non-positives with -inf so they don't affect max\n",
    "    masked_dist_pos = pairwise_dist.clone()\n",
    "    masked_dist_pos[~positive_mask] = torch.nan\n",
    "    \n",
    "    # For negatives: replace non-negatives with inf so they don't affect min\n",
    "    masked_dist_neg = pairwise_dist.clone()\n",
    "    masked_dist_neg[~negative_mask] = torch.nan\n",
    "    \n",
    "    # Compute hardest positives and negatives efficiently\n",
    "    # remoevc nan and inf etc from the tensor\n",
    "     \n",
    "    hardest_positives = torch.nanquantile(masked_dist_pos, 0.9, dim=1)  # 90th percentile distance to positive\n",
    "    hardest_negatives = torch.nanquantile(masked_dist_neg, 0.1, dim=1)  # 10th percentile distance to negative\n",
    "    \n",
    "    # Compute triplet loss with margin\n",
    "    triplet_losses = torch.clamp(hardest_positives - hardest_negatives + margin, min=0.0)\n",
    "    \n",
    "    # Only use valid triplets in final loss\n",
    "    return triplet_losses[valid_triplets].mean()\n",
    "# def compute_cluster_loss(z, labels, k=15):\n",
    "#     # Compute k-nearest neighbors graph in latent space\n",
    "#     knn_graph = kneighbors_graph(z.detach().cpu().numpy(), k, mode='connectivity', include_self=False)\n",
    "#     knn_graph = torch.from_numpy(knn_graph.toarray()).float().to(z.device)\n",
    "    \n",
    "#     # Compute cluster assignment matrix\n",
    "#     unique_labels = torch.unique(labels)\n",
    "#     cluster_assignment = torch.zeros(len(labels), len(unique_labels), device=z.device)\n",
    "#     for i, label in enumerate(unique_labels):\n",
    "#         cluster_assignment[:, i] = (labels == label).float().squeeze()\n",
    "#     # Compute within-cluster connectivity\n",
    "#     within_cluster_conn = torch.sum(knn_graph * torch.mm(cluster_assignment, cluster_assignment.t()))\n",
    "#     # Compute between-cluster connectivity\n",
    "#     between_cluster_conn = torch.sum(knn_graph * (1 - torch.mm(cluster_assignment, cluster_assignment.t())))\n",
    "#     # Compute loss (maximize within-cluster connectivity, minimize between-cluster connectivity)\n",
    "#     loss = -within_cluster_conn + between_cluster_conn\n",
    "    \n",
    "# 1. Set up your AnnData object with combined data\n",
    "COVET_SQRT = PCA(n_components=100).fit_transform(adata_2_prot.obsm['COVET_SQRT'])\n",
    "combined_data = np.concatenate([adata_2_prot.X.todense() if issparse(adata_2_prot.X) else adata_2_prot.X,\n",
    "                                 COVET_SQRT], axis=1)\n",
    "\n",
    "# combined_data = adata_2_prot\n",
    "combined_adata = AnnData(X=combined_data, obs=adata_2_prot.obs)\n",
    "combined_adata.obs['index_col'] = range(combined_adata.shape[0])  \n",
    "# scale all cols\n",
    "# sc.pp.scale(combined_adata)\n",
    "combined_adata.X = combined_adata.X - combined_adata.X.min()\n",
    "# sc.pp.normalize_total(combined_adata)\n",
    "combined_adata.X = np.abs(combined_adata.X)\n",
    "# 2. Set up scVI\n",
    "scvi.model.SCVI.setup_anndata(\n",
    "    combined_adata,\n",
    "    labels_key=\"index_col\",\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Create a standard scVI model with smaller architecture\n",
    "protein_vae = scvi.model.SCVI(\n",
    "    combined_adata,\n",
    "    n_hidden=50,  # Smaller hidden layers\n",
    "    n_latent=10,  # Latent space dimensionality\n",
    "    n_layers=1,   # Single layer for simplicity\n",
    "    gene_likelihood=\"normal\",  # Use normal distribution for non-count data\n",
    "    use_layer_norm=True,  # Add layer normalization\n",
    "    use_batch_norm=True,  # Add batch normalization\n",
    "\n",
    ")\n",
    "\n",
    "protein_vae._training_plan_cls = ClusterPreservingTrainingPlan\n",
    "num_epochs = 200\n",
    "batch_size = 1200\n",
    "\n",
    "sc.pp.pca( adata_2_prot,n_comps=10)\n",
    "target_silhouette = silhouette_avg = silhouette_score(adata_2_prot.obsm['X_pca'], adata_2_prot.obs['orig_prot_clusters'].astype(int))\n",
    "\n",
    "print(f\"Silhouette score of the original protein clustering: {target_silhouette}\")\n",
    "\n",
    "protein_vae.train(\n",
    "    max_epochs=num_epochs,\n",
    "    check_val_every_n_epoch=10,\n",
    "    early_stopping=False,\n",
    "    batch_size=batch_size,\n",
    "    plan_kwargs={\n",
    "        'protein_vae': protein_vae,\n",
    "        'cluster_labels': torch.tensor(combined_adata.obs['orig_prot_clusters'].values).long(),\n",
    "        'batch_size': batch_size,\n",
    "        'plot_x_times': 20,\n",
    "        'combined_adata': combined_adata,\n",
    "        'target_silhouette': 2*target_silhouette,  # Target silhouette score\n",
    "        'silhouette_tolerance': 0.05,  # Acceptable range around target\n",
    "        'cluster_weight': 200.0,  # Start with a moderate weight\n",
    "        'min_cluster_weight': 100,  # Minimum allowed weight\n",
    "        'max_cluster_weight': 2000.0   # Maximum allowed weight\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the combined data\n",
    "sc.pp.scale(combined_adata)\n",
    "combined_adata.X = combined_adata.X - combined_adata.X.min()\n",
    "\n",
    "sc.pp.pca(combined_adata, n_comps=50)\n",
    "\n",
    "# Compute the neighborhood graph using PCA representation\n",
    "sc.pp.neighbors(combined_adata, use_rep='X_pca')\n",
    "\n",
    "# Run UMAP for visualization\n",
    "sc.tl.umap(combined_adata)\n",
    "\n",
    "# Plot the UMAP embedding\n",
    "sc.pl.umap(combined_adata, color=['cell_types', 'orig_prot_clusters'], title=['UMAP - Cell Types', 'UMAP - Original Protein Clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Perform PCA on the combined data\n",
    "sc.pp.scale(combined_adata)\n",
    "combined_adata.X = combined_adata.X - combined_adata.X.min()\n",
    "\n",
    "sc.pp.pca(combined_adata, n_comps=50)\n",
    "\n",
    "# Compute the neighborhood graph using PCA representation\n",
    "sc.pp.neighbors(combined_adata, use_rep='X_pca')\n",
    "\n",
    "# Run UMAP for visualization\n",
    "sc.tl.umap(combined_adata)\n",
    "\n",
    "# Plot the UMAP embedding\n",
    "sc.pl.umap(combined_adata, color=['cell_types', 'orig_prot_clusters'], title=['UMAP - Cell Types', 'UMAP - Original Protein Clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bar_nick_utils import plot_normalized_losses\n",
    "\n",
    "\n",
    "plot_normalized_losses(protein_vae.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata,adata_2_prot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming the model is trained and latent representation is extracted\n",
    "latent_prot = protein_vae.get_latent_representation()\n",
    "# Add latent representation to AnnData object\n",
    "combined_adata.obsm['X_scVI'] = latent_prot\n",
    "# Compute neighborhood graph\n",
    "sc.pp.neighbors(combined_adata, use_rep='X_scVI',key_added='scVI_neighbors')\n",
    "sc.tl.umap(combined_adata,neighbors_key='scVI_neighbors')\n",
    "sc.pl.umap(combined_adata, color=['cell_types','orig_prot_clusters'], title=['latent - Cell Types', 'latent - original prot Clustering'])\n",
    "# sc.pl.umap(combined_adata, color=combined_adata.var_names[:5], ncols=3, title='UMAP of Protein Latent Space - Top 5 Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "sc.pp.pca(adata_2_prot, n_comps=10)\n",
    "sc.pp.neighbors(adata_2_prot, use_rep='X_pca')\n",
    "sc.pl.umap(adata_2_prot, color=['cell_types', 'orig_prot_clusters'], title=['UMAP of Original Protein Data - Cell Types', \n",
    "                                                                            'UMAP of Original Protein Data - Original Protein Clustering'],\n",
    "                                                                            neighbors_key='neighbors')\n",
    "# Identify the largest cluster in 'orig_prot_clusters'\n",
    "largest_cluster = adata_2_prot.obs['orig_prot_clusters'].value_counts().idxmax()\n",
    "\n",
    "# Subset the data to include only the largest cluster\n",
    "adata_largest_cluster = adata_2_prot[adata_2_prot.obs['orig_prot_clusters'] == largest_cluster]\n",
    "\n",
    "# Plot UMAP for the largest cluster, colored by 'cell_types'\n",
    "sc.pl.umap(adata_largest_cluster, color='cell_types', title=f'Largest Cluster ({largest_cluster}) - Colored by Cell Types')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "silhouette_avg = silhouette_score(combined_adata.obsm['X_scVI'],\n",
    "                                  combined_adata.obs['orig_prot_clusters'].astype(int))\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "'a'+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_2_prot.layers[\"original_data\"] = adata_2_prot.X.copy()\n",
    "adata_2_prot = AnnData(combined_adata.obsm['X_scVI'], obs=combined_adata.obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the old way of combineing the protein expressin and the COVET, no longer needed\n",
    "# Assuming `adata_prot` is the original AnnData object\n",
    "# And `neighbor_means` is the new matrix to be concatenated\n",
    "# different adata\n",
    "# new_feature_names = [f\"CN_{i}\" for i in range(covet_adata.shape[1])]\n",
    "# covet_adata_dense = covet_adata.X.toarray() if issparse(covet_adata.X) else covet_adata.X\n",
    "# # sc.pp.pca(covet_adata, n_comps=20)\n",
    "# adata_2_prot.X = adata_2_prot.X.toarray() if issparse(adata_2_prot.X) else adata_2_prot.X\n",
    "# new_X = np.hstack([adata_2_prot.obsm['X_pca']\n",
    "#                    , covet_adata_dense])\n",
    "# additional_var = pd.DataFrame(index=new_feature_names)\n",
    "# prot_vars_pca =   [f\"_{i}\" for i in range(adata_2_prot.obsm['X_pca'].shape[1])]\n",
    "# prot_vars_pca = pd.DataFrame(index=prot_vars_pca)\n",
    "# new_vars = pd.concat([prot_vars_pca, additional_var])\n",
    "# new_vars['higly_variable'] = True\n",
    "# new_vars['n_cells'] = covet_adata.shape[0]\n",
    "# adata_2_prot_new = anndata.AnnData(\n",
    "#     X=new_X,\n",
    "#     obs=adata_2_prot.obs.copy(),  # Keep the same observation metadata\n",
    "#     var=new_vars,  # Keep the same variable metadata\n",
    "#     uns=adata_2_prot.uns.copy(),  # Keep the same unstructured data #todo brin back?\n",
    "#     obsm=adata_2_prot.obsm.copy(),  # Keep the same observation matrices ? #todo bring back?\n",
    "#     # varm=adata_prot.varm.copy(), # Keep the same variable matrices\n",
    "#     # layers=adata_2_prot.layers.copy()  # Keep the same layers\n",
    "# )\n",
    "# prot_feat_num = prot_vars_pca.shape[0]\n",
    "# adata_2_prot_new.var['feature_type'] = ['protein'] * prot_feat_num + ['CN'] * covet_adata.shape[1]\n",
    "# # plot pca variance using scanpy\n",
    "# # remove pca\n",
    "# if 'X_pca' in adata_2_prot_new.obsm:\n",
    "#     adata_2_prot_new.obsm.pop('X_pca')\n",
    "# #  set new highly variable genes\n",
    "# adata_2_prot_new.var['highly_variable'] = True\n",
    "# # adata_2_prot_new.obs['CN'] = adata_2_prot.obs['CN']\n",
    "# adata_2_prot_new.X = (zscore(adata_2_prot_new.X, axis=0))\n",
    "# sc.pp.highly_variable_genes(adata_2_prot_new)\n",
    "# sc.pp.pca(adata_2_prot_new)\n",
    "# sc.pp.neighbors(adata_2_prot_new, n_neighbors=15)\n",
    "# sc.tl.umap(adata_2_prot_new)\n",
    "# sc.pl.umap(adata_2_prot_new, color='CN', title='UMAP of CN embedding to make sure they are not mixed')\n",
    "# # todo make sure no more things to remove\n",
    "# new_pca = adata_2_prot_new.obsm['X_pca']\n",
    "# sns.heatmap(adata_2_prot_new.X)\n",
    "# sc.pl.pca_variance_ratio(adata_2_prot_new, log=True, n_pcs=50, save='.pdf')\n",
    "# sc.pp.pca(adata_2_prot_new)\n",
    "# if 'adata_2_prot_old' not in globals():\n",
    "#     adata_2_prot_old = adata_2_prot.copy()\n",
    "# print(f\"New adata shape (protein features + cell neighborhood vector): {adata_2_prot_new.shape}\")\n",
    "# # make sure adata2 prot is float32 and dense mat\n",
    "# adata_2_prot_new.X = adata_2_prot_new.X.astype('float32')\n",
    "# adata_2_prot_new.X = adata_2_prot_new.X.toarray() if issparse(adata_2_prot_new.X) else adata_2_prot_new.X\n",
    "# adata_2_prot = adata_2_prot_new  # todo uncomment this\n",
    "# todo why this crash the num of dim make not sence \n",
    "# adata_2_prot_new = AnnData(adata_2_prot_new.X, obs=adata_2_prot_new.obs, var=adata_2_prot_new.var)\n",
    "# max_possible_pca_dim_prot = min(adata_2_prot_new.X.shape[1], adata_2_prot_new.X.shape[0])\n",
    "# # sc.pp.pca(adata_2_prot_new, n_comps=max_possible_pca_dim_prot - 1)\n",
    "# cumulative_variance_ratio = np.cumsum(adata_2_prot_new.uns['pca']['variance_ratio'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # End of CN concatenation to protein features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different rna, protein data analysis\n",
    "sc.pp.pca(adata_1_rna)\n",
    "sc.pp.pca(adata_2_prot)\n",
    "sc.pp.neighbors(adata_1_rna, key_added='original_neighbors', use_rep='X_pca')\n",
    "sc.tl.umap(adata_1_rna, neighbors_key='original_neighbors')\n",
    "adata_1_rna.obsm['X_original_umap'] = adata_1_rna.obsm[\"X_umap\"]\n",
    "sc.pp.neighbors(adata_2_prot, key_added='original_neighbors', use_rep='X_pca')\n",
    "sc.tl.umap(adata_2_prot, neighbors_key='original_neighbors')\n",
    "adata_2_prot.obsm['X_original_umap'] = adata_2_prot.obsm[\"X_umap\"]\n",
    "\n",
    "if plot_flag:\n",
    "    sc.tl.umap(adata_2_prot, neighbors_key='original_neighbors')\n",
    "    sc.pl.pca(adata_1_rna, color=['cell_types', 'major_cell_types'],\n",
    "              title=['RNA pca minor cell types', 'RNA pca major cell types'])\n",
    "    sc.pl.pca(adata_2_prot, color=['cell_types', 'major_cell_types'],\n",
    "              title=['Protein pca minor cell types', 'Protein pca major cell types'])\n",
    "    sc.pl.embedding(adata_1_rna, basis='X_umap', color=['major_cell_types', 'cell_types'],\n",
    "                    title=['RNA UMAP major cell types', 'RNA UMAP major cell types'])\n",
    "    sc.pl.embedding(adata_2_prot, basis='X_original_umap', color=['major_cell_types', 'cell_types'],\n",
    "                    title=['Protein UMAp major cell types', 'Protein UMAP major cell types'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_2_prot.varm['PCs'].shape\n",
    "adata_2_prot.shape\n",
    "# Plot the explained variance ratio using scanpy's built-in function\n",
    "sc.pl.pca_variance_ratio(adata_2_prot, log=True, n_pcs=10)\n",
    "# pcs variance\n",
    "pcs_variance = adata_2_prot.uns['pca']['variance_ratio']\n",
    "print(f\"Explained variance ratio of the first 10 PCs: {pcs_variance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# import networkx as nx\n",
    "# from node2vec import Node2Vec\n",
    "# # Load your RNA expression data\n",
    "# # Assuming 'adata' is your AnnData object\n",
    "# X = adata.X\n",
    "# # Compute kNN graph\n",
    "# nn = NearestNeighbors(n_neighbors=10)\n",
    "# nn.fit(X)\n",
    "# distances, indices = nn.kneighbors(X)\n",
    "# # Create adjacency matrix\n",
    "# adj = np.zeros((X.shape[0], X.shape[0]))\n",
    "# for i in range(X.shape[0]):\n",
    "#     adj[i, indices[i]] = 1\n",
    "# # Create a NetworkX graph\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(range(X.shape[0]))\n",
    "# for i in range(X.shape[0]):\n",
    "#     for j in indices[i]:\n",
    "#         G.add_edge(i, j)\n",
    "# # Apply Node2Vec\n",
    "# node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "# model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "# # Get node embeddings\n",
    "# node_embeddings = model.wv.vectors\n",
    "# # Use the embeddings for further analysis\n",
    "# print(node_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing a gene module score \n",
    "terminal_exhaustion = [\n",
    "    \"CD3G\", \"FASLG\", \"ID2\", \"LAG3\", \"RGS1\",\n",
    "    \"CCL3\", \"CCL3L1\", \"KIAA1671\", \"SH2D2A\", \"DUSP2\",\n",
    "    \"PDCD1\", \"CD7\", \"NR4A2\", \"CD160\", \"PTPN22\",\n",
    "    \"ABI3\", \"PTGER4\", \"GZMK\", \"GZMA\", \"MBNL1\",\n",
    "    \"VMP1\", \"PLAC8\", \"RGS3\", \"EFHD2\", \"GLRX\",\n",
    "    \"CXCR6\", \"ARL6IP1\", \"CCL4\", \"ISG15\", \"LAX1\",\n",
    "    \"CD8A\", \"SERPINA3\", \"GZMB\", \"TOX\"\n",
    "]\n",
    "precursor_exhaustion = [\n",
    "    \"TCF7\", \"MS4A4A\", \"TNFSF8\", \"CXCL10\", \"EEF1B2\",\n",
    "    \"ID3\", \"IL7R\", \"JUN\", \"LTB\", \"XCL1\",\n",
    "    \"SOCS3\", \"TRAF1\", \"EMB\", \"CRTAM\", \"EEF1G\",\n",
    "    \"CD9\", \"ITGB1\", \"GPR183\", \"ZFP36L1\", \"SLAMF6\",\n",
    "    \"LY6E\"\n",
    "]\n",
    "cd8_t_cell_activation = [\n",
    "    \"CD69\", \"CCR7\", \"CD27\", \"BTLA\", \"CD40LG\",\n",
    "    \"IL2RA\", \"CD3E\", \"CD47\", \"EOMES\", \"GNLY\",\n",
    "    \"GZMA\", \"GZMB\", \"PRF1\", \"IFNG\", \"CD8A\",\n",
    "    \"CD8B\", \"CD95L\", \"LAMP1\", \"LAG3\", \"CTLA4\",\n",
    "    \"HLA-DRA\", \"TNFRSF4\", \"ICOS\", \"TNFRSF9\", \"TNFRSF18\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_1_rna.var_names = adata_1_rna.var_names.str.upper()\n",
    "adata_2_prot.var_names = adata_2_prot.var_names.str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.score_genes(adata_1_rna, gene_list=terminal_exhaustion, score_name=\"terminal_exhaustion_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flag = True\n",
    "if plot_flag:\n",
    "    sc.pl.umap(adata_1_rna, color=\"terminal_exhaustion_score\", cmap=\"viridis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_1_rna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### analysis to get to scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # different adata analysis\n",
    "# adata_2_prot_first_110_vars = adata_2_prot[:, adata_2_prot.var_names[:110]].copy()\n",
    "# adata_2_prot_second_110_vars = adata_2_prot[:, adata_2_prot.var_names[110:]].copy()\n",
    "# sc.pp.pca(adata_2_prot_first_110_vars)\n",
    "# sc.pp.pca(adata_2_prot_second_110_vars)\n",
    "# # plot umap each separately\n",
    "# sc.pp.neighbors(adata_2_prot_first_110_vars)\n",
    "# sc.tl.umap(adata_2_prot_first_110_vars)\n",
    "# sc.pp.neighbors(adata_2_prot_first_110_vars)\n",
    "# sc.tl.umap(adata_2_prot_first_110_vars)\n",
    "# sc.pp.neighbors(adata_2_prot_second_110_vars)\n",
    "# sc.tl.umap(adata_2_prot_second_110_vars)\n",
    "\n",
    "# if plot_flag:\n",
    "#     sc.pl.embedding(adata_2_prot_first_110_vars, basis='X_umap', color=[ 'major_cell_types','cell_types'], title=['Protein UMAP first 110 vars major cell types','Protein UMAP first 110 vars major cell types'])\n",
    "#     sc.pl.embedding(adata_2_prot_second_110_vars, basis='X_umap', color=[ 'major_cell_types','cell_types'], title=['Protein UMAP second 110 vars major cell types','Protein UMAP second 110 vars major cell types'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adata_2_prot.uns['pca']['variance_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# different rna, protein adata analysis\n",
    "max_possible_pca_dim_rna = min(adata_1_rna.X.shape[1], adata_1_rna.X.shape[0])\n",
    "max_possible_pca_dim_prot = min(adata_2_prot.X.shape[1], adata_2_prot.X.shape[0])\n",
    "sc.pp.pca(adata_1_rna, n_comps=max_possible_pca_dim_rna - 1)\n",
    "# sc.pp.pca(adata_2_prot, n_comps=max_possible_pca_dim_prot - 1)\n",
    "# make PCA explain X% of variance\n",
    "\n",
    "# going to make pca 25 here just so they have the same number of pca\n",
    "max_dim = 50\n",
    "variance_ration_selected = 0.75\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(adata_1_rna.uns['pca']['variance_ratio'])\n",
    "n_comps_thresh = np.argmax(cumulative_variance_ratio >= variance_ration_selected) + 1\n",
    "n_comps_thresh = min(n_comps_thresh, max_dim)\n",
    "if n_comps_thresh == 1:\n",
    "    raise ValueError('n_comps_thresh is 1, this is not good, try to lower the variance_ration_selected')\n",
    "real_ratio = np.cumsum(adata_1_rna.uns['pca']['variance_ratio'])[n_comps_thresh]\n",
    "# sc.pp.pca(adata_1_rna, n_comps=n_comps_thresh)\n",
    "sc.pp.pca(adata_1_rna, n_comps=n_comps_thresh)\n",
    "print(f\"\\nNumber of components explaining {real_ratio} of rna variance: {n_comps_thresh}\\n\")\n",
    "sc.pp.pca(adata_2_prot)\n",
    "variance_ration_selected = 0.999\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(adata_2_prot.uns['pca']['variance_ratio'])\n",
    "n_comps_thresh = np.argmax(cumulative_variance_ratio >= variance_ration_selected) + 1\n",
    "n_comps_thresh = min(n_comps_thresh, max_dim)\n",
    "real_ratio = np.cumsum(adata_2_prot.uns['pca']['variance_ratio'])[n_comps_thresh]\n",
    "# sc.pp.pca(adata_2_prot, n_comps=n_comps_thresh)\n",
    "sc.pp.pca(adata_1_rna, n_comps=n_comps_thresh)\n",
    "print(f\"\\nNumber of components explaining {real_ratio} of protein variance: {n_comps_thresh}\")\n",
    "if n_comps_thresh == 1:\n",
    "    raise ValueError('n_comps_thresh is 1, this is not good, try to lower the variance_ration_selected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find Archetypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different adata analysis\n",
    "archetype_list_protein = []\n",
    "archetype_list_rna = []\n",
    "converge = 1e-5\n",
    "min_k = 9\n",
    "max_k = 10\n",
    "step_size = 1\n",
    "\n",
    "# Store explained variances for plotting the elbow method\n",
    "evs_protein = []\n",
    "evs_rna = []\n",
    "\n",
    "# Protein archetype detection\n",
    "X_protein = adata_2_prot.obsm['X_pca'].T\n",
    "total = (max_k - min_k) / step_size\n",
    "for i, k in tqdm(enumerate(range(min_k, max_k, step_size)), total=total, desc='Protein Archetypes Detection'):\n",
    "    archetype, _, _, _, ev = PCHA(X_protein, noc=k)\n",
    "    evs_protein.append(ev)\n",
    "    archetype_list_protein.append(np.array(archetype).T)\n",
    "    if i > 0 and evs_protein[i] - evs_protein[i - 1] < converge:\n",
    "        print('Early stopping for Protein')\n",
    "        break\n",
    "\n",
    "# RNA archetype detection\n",
    "X_rna = adata_1_rna.obsm['X_pca'].T\n",
    "for j, k in tqdm(enumerate(range(min_k, max_k, step_size)), total=total, desc='RNA Archetypes Detection'):\n",
    "    if j > i:\n",
    "        break\n",
    "    archetype, _, _, _, ev = PCHA(X_rna, noc=k)\n",
    "    evs_rna.append(ev)\n",
    "    archetype_list_rna.append(np.array(archetype).T)\n",
    "    if j > 0 and evs_rna[j] - evs_rna[j - 1] < converge:\n",
    "        print('Early stopping for RNA')\n",
    "        break\n",
    "\n",
    "# Ensure both lists have the same length\n",
    "min_len = min(len(archetype_list_protein), len(archetype_list_rna))\n",
    "archetype_list_protein = archetype_list_protein[:min_len]\n",
    "archetype_list_rna = archetype_list_rna[:min_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow Method for both protein and RNA\n",
    "if plot_flag:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ks = list(range(min_k, min_k + len(evs_protein)))  # Adjust for early stopping\n",
    "\n",
    "    plt.plot(ks, evs_protein, marker='o', label='Protein EV')\n",
    "    plt.plot(ks[:len(evs_rna)], evs_rna, marker='s', label='RNA EV')\n",
    "    plt.xlabel(\"Number of Archetypes (k)\")\n",
    "    plt.ylabel(\"Explained Variance\")\n",
    "    plt.title(\"Elbow Method for Archetype Selection\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different sample analysis\n",
    "minor_cell_types_list_prot = sorted(list(set(adata_2_prot.obs['cell_types'])))\n",
    "major_cell_types_list_prot = sorted(list(set(adata_2_prot.obs['major_cell_types'])))\n",
    "\n",
    "# have to do this above two lines for rna too\n",
    "minor_cell_types_list_rna = sorted(list(set(adata_1_rna.obs['cell_types'])))\n",
    "major_cell_types_list_rna = sorted(list(set(adata_1_rna.obs['major_cell_types'])))\n",
    "\n",
    "major_cell_types_amount_prot = [adata_2_prot.obs['major_cell_types'].value_counts()[cell_type] for cell_type in\n",
    "                                major_cell_types_list_prot]\n",
    "major_cell_types_amount_rna = [adata_1_rna.obs['major_cell_types'].value_counts()[cell_type] for cell_type in\n",
    "                               major_cell_types_list_rna]\n",
    "assert set(adata_1_rna.obs['major_cell_types']) == set(adata_2_prot.obs['major_cell_types'])\n",
    "archetype_proportion_list_rna, archetype_proportion_list_protein = [], []\n",
    "\n",
    "# i made sure that archetype prot and archetype rna have the same dimensions but still not working\n",
    "\n",
    "for archetypes_prot, archetypes_rna in tqdm(zip(archetype_list_protein, archetype_list_rna),\n",
    "                                            total=len(archetype_list_protein),\n",
    "                                            desc='Archetypes generating archetypes major cell types proportion vector '):\n",
    "    weights_prot = get_cell_representations_as_archetypes_cvxpy(adata_2_prot.obsm['X_pca'], archetypes_prot)\n",
    "    weights_rna = get_cell_representations_as_archetypes_cvxpy(adata_1_rna.obsm['X_pca'], archetypes_rna)\n",
    "\n",
    "    archetypes_dim_prot = archetypes_prot.shape[1]  # these dimensions are not 25 anymore, made them 50\n",
    "    archetype_num_prot = archetypes_prot.shape[0]\n",
    "    # need to do the above two lines for rna too\n",
    "\n",
    "    # archetype num prot and rna should be the same?\n",
    "    archetypes_dim_rna = archetypes_rna.shape[1]  # these dimensions are 50\n",
    "    archetype_num_rna = archetypes_rna.shape[0]\n",
    "\n",
    "    # could it be because the minor and major cell types have different lengths for protein and rna?\n",
    "    prot_arch_prop = pd.DataFrame(np.zeros((archetype_num_prot, len(major_cell_types_list_prot))),\n",
    "                                  columns=major_cell_types_list_prot)\n",
    "    rna_arch_prop = pd.DataFrame(np.zeros((archetype_num_rna, len(major_cell_types_list_rna))),\n",
    "                                 columns=major_cell_types_list_rna)\n",
    "    archetype_cell_proportions = np.zeros((archetype_num_prot, len(major_cell_types_list_rna)))\n",
    "    for curr_archetype in range(archetype_num_prot):\n",
    "        df_rna = pd.DataFrame([weights_prot[:, curr_archetype], adata_2_prot.obs['major_cell_types'].values],\n",
    "                              index=['weight', 'major_cell_types']).T\n",
    "        df_prot = pd.DataFrame([weights_rna[:, curr_archetype], adata_1_rna.obs['major_cell_types'].values],\n",
    "                               index=['weight', 'major_cell_types']).T\n",
    "        df_rna = df_rna.groupby('major_cell_types')['weight'].sum()[major_cell_types_list_rna]\n",
    "        df_prot = df_prot.groupby('major_cell_types')['weight'].sum()[major_cell_types_list_prot]\n",
    "        # normalize by the amount of major cell types\n",
    "        rna_arch_prop.loc[curr_archetype, :] = df_rna.values / major_cell_types_amount_rna\n",
    "        prot_arch_prop.loc[curr_archetype, :] = df_prot.values / major_cell_types_amount_prot\n",
    "\n",
    "    prot_arch_prop = (prot_arch_prop.T / prot_arch_prop.sum(1)).T\n",
    "    prot_arch_prop = prot_arch_prop / prot_arch_prop.sum(0)\n",
    "    rna_arch_prop = (rna_arch_prop.T / rna_arch_prop.sum(1)).T\n",
    "    rna_arch_prop = rna_arch_prop / rna_arch_prop.sum(0)\n",
    "    archetype_proportion_list_rna.append(rna_arch_prop.copy())\n",
    "    archetype_proportion_list_protein.append(prot_arch_prop.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths of major cell type amount rna and protein are the same\n",
    "print(major_cell_types_amount_rna)\n",
    "print(major_cell_types_amount_prot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results of the lowest num of archetypes\n",
    "if plot_flag:\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # sns.heatmap(reorder_rows_to_maximize_diagonal(archetype_proportion_list_rna[0])[0])\n",
    "    sns.heatmap((archetype_proportion_list_rna[0]), cbar=False)\n",
    "    plt.xticks()\n",
    "    plt.title('RNA Archetypes')\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Protein Archetypes')\n",
    "    # sns.heatmap(reorder_rows_to_maximize_diagonal(archetype_proportion_list_protein[0])[0])\n",
    "    sns.heatmap((archetype_proportion_list_protein[0]), cbar=False)\n",
    "    plt.suptitle('showcase the relationship between archetypes and cell types')\n",
    "    plt.yticks([])\n",
    "    plt.suptitle('Non-Aligned Archetypes Profiles')\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.show()\n",
    "\n",
    "    new_order_1 = reorder_rows_to_maximize_diagonal(archetype_proportion_list_rna[0])[1]\n",
    "    new_order_2 = reorder_rows_to_maximize_diagonal(archetype_proportion_list_protein[0])[1]\n",
    "    data1 = archetype_proportion_list_rna[0].iloc[new_order_1, :]\n",
    "    data2 = archetype_proportion_list_protein[0].iloc[new_order_2, :]\n",
    "    # this just uses simple diagonal optimization for each one separatly, this is not final matching\n",
    "    plot_archetypes_matching(data1, data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo find_best_pair_by_row_matching() find a mathing which has too many archetypes, should check it later\n",
    "best_num_or_archetypes_index, best_total_cost, best_rna_archetype_order, best_protein_archetype_order = find_best_pair_by_row_matching(\n",
    "    copy.deepcopy(archetype_proportion_list_rna), copy.deepcopy(archetype_proportion_list_protein), metric='correlation'\n",
    ")\n",
    "\n",
    "print(\"\\nBest pair found:\")\n",
    "print(f\"Best index: {best_num_or_archetypes_index}\")\n",
    "print(f\"Best total matching cost: {best_total_cost}\")\n",
    "print(f\"Row indices (RNA): {best_rna_archetype_order}\")\n",
    "print(f\"Matched row indices (Protein): {best_protein_archetype_order}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the archetypes based on the best matching so the archtypes across modalities are aligned\n",
    "best_archetype_rna_prop = archetype_proportion_list_rna[best_num_or_archetypes_index].iloc[\n",
    "                          best_rna_archetype_order, :].reset_index(drop=True)\n",
    "# best_archetype_rna_prop = pd.DataFrame(best_archetype_rna_prop)\n",
    "best_archetype_prot_prop = archetype_proportion_list_protein[best_num_or_archetypes_index].iloc[\n",
    "                           best_protein_archetype_order, :].reset_index(drop=True)\n",
    "# best_archetype_prot_prop = pd.DataFrame(best_archetype_prot_prop)\n",
    "if plot_flag:\n",
    "    plot_archetypes_matching(best_archetype_rna_prop, best_archetype_prot_prop, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_archetype_prot_prop.idxmax(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    # show the proportion of each cell type in the archetypes, for both modalities, to see the overlap higer overlap is better\n",
    "    best_archetype_prot_prop.idxmax(axis=0).plot(kind='bar', color='red', hatch='\\\\', label='Protein')\n",
    "    best_archetype_rna_prop.idxmax(axis=0).plot(kind='bar', alpha=0.5, hatch='/', label='RNA')\n",
    "    plt.title('show overlap of cell types proportions in archetypes')\n",
    "    # add legend\n",
    "    plt.legend()\n",
    "    plt.xlabel('Major Cell Types')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    compare_matchings(archetype_proportion_list_rna, archetype_proportion_list_protein, metric='cosine',\n",
    "                      num_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_protein_archetype_order\n",
    "adata_2_prot.obsm['X_pca'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cells archetype vec and see how they match\n",
    "ordered_best_rna_archetype = archetype_list_rna[best_num_or_archetypes_index][best_protein_archetype_order, :]\n",
    "ordered_best_protein_archetype = archetype_list_protein[best_num_or_archetypes_index][best_rna_archetype_order, :]\n",
    "cells_archetype_vec_rna = get_cell_representations_as_archetypes_cvxpy(adata_1_rna.obsm['X_pca'],\n",
    "                                                                       ordered_best_rna_archetype)\n",
    "cells_archetype_vec_prot = get_cell_representations_as_archetypes_cvxpy(adata_2_prot.obsm['X_pca'],\n",
    "                                                                        ordered_best_protein_archetype)\n",
    "\n",
    "adata_1_rna.obsm['archetype_vec'] = pd.DataFrame(cells_archetype_vec_rna, index=adata_1_rna.obs.index,\n",
    "                                                 columns=range(cells_archetype_vec_rna.shape[1]))\n",
    "adata_2_prot.obsm['archetype_vec'] = pd.DataFrame(cells_archetype_vec_prot, index=adata_2_prot.obs.index,\n",
    "                                                  columns=range(cells_archetype_vec_prot.shape[1]))\n",
    "adata_1_rna.obsm['archetype_vec'].columns = adata_1_rna.obsm['archetype_vec'].columns.astype(str)\n",
    "adata_2_prot.obsm['archetype_vec'].columns = adata_2_prot.obsm['archetype_vec'].columns.astype(str)\n",
    "\n",
    "adata_1_rna.obs['archetype_label'] = pd.Categorical(np.argmax(cells_archetype_vec_rna, axis=1))\n",
    "adata_2_prot.obs['archetype_label'] = pd.Categorical(np.argmax(cells_archetype_vec_prot, axis=1))\n",
    "adata_1_rna.uns['archetypes'] = ordered_best_rna_archetype\n",
    "adata_2_prot.uns['archetypes'] = ordered_best_protein_archetype\n",
    "metrics = ['euclidean', 'cityblock', 'cosine', 'correlation', 'chebyshev']\n",
    "evaluate_distance_metrics(cells_archetype_vec_rna, cells_archetype_vec_prot, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the best matching archetype to the metadata\n",
    "adata_1_rna.obs['archetype_label'] = pd.Categorical(np.argmax(cells_archetype_vec_rna, axis=1))\n",
    "adata_2_prot.obs['archetype_label'] = pd.Categorical(np.argmax(cells_archetype_vec_prot, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_archetype_rna = AnnData(adata_1_rna.obsm['archetype_vec'])\n",
    "adata_archetype_prot = AnnData(adata_2_prot.obsm['archetype_vec'])\n",
    "adata_archetype_rna.obs = adata_1_rna.obs\n",
    "adata_archetype_prot.obs = adata_2_prot.obs\n",
    "adata_archetype_rna.index = adata_1_rna.obs.index\n",
    "adata_archetype_prot.index = adata_2_prot.obs.index\n",
    "# save all adata objects with time stamp\n",
    "clean_uns_for_h5ad(adata_2_prot)\n",
    "clean_uns_for_h5ad(adata_1_rna)\n",
    "time_stamp = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "adata_1_rna.write(f'data/adata_rna_{time_stamp}.h5ad')\n",
    "adata_2_prot.write(f'data/adata_prot_{time_stamp}.h5ad')\n",
    "adata_archetype_rna.write(f'data/adata_archetype_rna_{time_stamp}.h5ad')\n",
    "adata_archetype_prot.write(f'data/adata_archetype_prot_{time_stamp}.h5ad')\n",
    "# load the latest of each sort by time as if I dont have the time stamp, read all files in the right name prefix and sort by time in a folder\n",
    "folder = 'data/'\n",
    "file_prefixes = ['adata_rna_', 'adata_prot_', 'adata_archetype_rna_', 'adata_archetype_prot_']\n",
    "\n",
    "# Load the latest files (example)\n",
    "latest_files = {prefix: get_latest_file(folder, prefix) for prefix in file_prefixes}\n",
    "adata_rna = sc.read(latest_files['adata_rna_'])\n",
    "adata_prot = sc.read(latest_files['adata_prot_'])\n",
    "adata_archetype_rna = sc.read(latest_files['adata_archetype_rna_'])\n",
    "adata_archetype_prot = sc.read(latest_files['adata_archetype_prot_'])\n",
    "print(latest_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.xlabel('PCA dimensiton of archetypes')\n",
    "    plt.title('RNA Archetypes')\n",
    "    sns.heatmap(ordered_best_rna_archetype)\n",
    "    plt.xlabel('PCA dimensiton of archetypes')\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.xlabel('PCA dimensiton of archetypes')\n",
    "    plt.title('Protein Archetypes')\n",
    "    sns.heatmap(ordered_best_protein_archetype)\n",
    "    plt.xlabel('PCA dimensiton of archetypes')\n",
    "    plt.ylabel('Archetypes')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('RNA Archetypes')\n",
    "    plt.ylabel('Archetypes')\n",
    "    _, row_order = reorder_rows_to_maximize_diagonal(best_archetype_rna_prop)\n",
    "    sns.heatmap(pd.DataFrame(best_archetype_rna_prop).iloc[row_order], cbar=False)\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.ylabel('Archetypes')\n",
    "    plt.title('Protein Archetypes')\n",
    "    sns.heatmap(pd.DataFrame(best_archetype_prot_prop).iloc[row_order], cbar=False)\n",
    "    plt.ylabel('Archetypes')\n",
    "    # plt.suptitle('The more similar the better, means that the archetypes are aligned in explaining different cell types')\n",
    "    plt.suptitle('Aligned Archetypes Profiles')\n",
    "    plt.yticks([])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('RNA Archetypes')\n",
    "plt.ylabel('Archetypes')\n",
    "_, row_order = reorder_rows_to_maximize_diagonal(best_archetype_rna_prop)\n",
    "sns.heatmap(pd.DataFrame(best_archetype_rna_prop).iloc[row_order], cbar=False)\n",
    "plt.yticks([])\n",
    "plt.ylabel('Archetypes')\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.ylabel('Archetypes')\n",
    "plt.title('Protein Archetypes')\n",
    "sns.heatmap(pd.DataFrame(best_archetype_prot_prop).iloc[row_order], cbar=False)\n",
    "plt.ylabel('Archetypes')\n",
    "# plt.suptitle('The more similar the better, means that the archetypes are aligned in explaining different cell types')\n",
    "plt.suptitle('Aligned Archetypes Profiles')\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archetype_distances = cdist(cells_archetype_vec_rna,\n",
    "                            cells_archetype_vec_prot, metric='correlation')\n",
    "if plot_flag:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(np.log1p(archetype_distances[:1000, :1000]))\n",
    "    plt.title(\n",
    "        'if diagonal bias this mean that the \\nmathing is somewhat correct (remember that \\ncells are sorted by tyeps for better visualization ')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # plt.plot(archetype_distances.argmin(axis=0))\n",
    "    plt.title('If this looks like line, matching \\nARE THE SAME AND NOT ACROSS MODALITIES')\n",
    "    min_values_highlight = archetype_distances.copy()\n",
    "    min_values_highlight[archetype_distances.argmin(axis=0), range(len(archetype_distances.argmin(axis=0)))] = 100\n",
    "    sns.heatmap(min_values_highlight[:5000, :5000])\n",
    "    # sns.heatmap(np.log1p(archetype_distances[:100,:100]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    ax = sns.histplot(adata_1_rna.obs, x='archetype_label', hue='major_cell_types', multiple='fill', stat='proportion')\n",
    "    plt.xticks(rotation=45)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.title('Proportion of Archetypes in Major Cell Types  in RNA')\n",
    "    plt.show()\n",
    "\n",
    "    ax = sns.histplot(adata_2_prot.obs, x='archetype_label', hue='major_cell_types', multiple='fill', stat='proportion')\n",
    "    plt.xticks(rotation=45)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.title('Proportion of Archetypes in Major Cell Types in Protein')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(mtx1, mtx2, n_samples):\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_combined = np.vstack((mtx1, mtx2))\n",
    "    tsne_results = pca.fit_transform(embeddings_combined)\n",
    "    # tsne_results = pca.fit_transform(embeddings_combined)\n",
    "\n",
    "    labels = ['Dataset 1'] * n_samples + ['Dataset 2'] * n_samples\n",
    "    df = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])\n",
    "    df['Dataset'] = labels\n",
    "\n",
    "    sns.scatterplot(x='TSNE1', y='TSNE2', hue='Dataset', data=df)\n",
    "    plt.title('t-SNE of Aligned Embeddings')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    # plot both heatmaps as subplots, and add titel the more similar the better:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('RNA Archetypes')\n",
    "    plt.ylabel('Archetypes')\n",
    "    _, row_order = reorder_rows_to_maximize_diagonal(best_archetype_rna_prop)\n",
    "    sns.heatmap(pd.DataFrame(best_archetype_rna_prop).loc[row_order, :])\n",
    "    # sns.heatmap(pd.DataFrame(best_archetype_rna_prop))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.ylabel('Archetypes')\n",
    "    # sns.heatmap(best_archetype_prot_prop)\n",
    "    sns.heatmap(pd.DataFrame(best_archetype_prot_prop).loc[row_order, :])\n",
    "    # sns.heatmap(pd.DataFrame(best_archetype_prot_prop).iloc[row_order,:])\n",
    "    plt.title('Protein Archetypes')\n",
    "    plt.suptitle('The more similar the better, means that the archtypes are aligned in explaining different cell types')\n",
    "    plt.show()\n",
    "    # errors = np.abs(ordered_arch_prot - ordered_arch_rna)\n",
    "    # random_error =np.abs(ordered_arch_prot - np.random.permutation(ordered_arch_rna))\n",
    "    # plt.plot(errors.values.flatten())\n",
    "    # plt.plot(random_error.values.flatten())\n",
    "    # plt.legend(['Error', 'Random Error'])\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot for RNA\n",
    "ax = sns.histplot(adata_1_rna.obs, x='archetype_label', hue='major_cell_types', multiple='fill', stat='proportion',\n",
    "                  ax=axes[0])\n",
    "axes[0].set_xticks(axes[0].get_xticks())  # Set the ticks first\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
    "axes[0].set_title('Proportion of Archetypes in Major Cell Types in RNA')\n",
    "axes[0].set_xlabel('Archetype Label')\n",
    "axes[0].tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)\n",
    "axes[0].get_legend().remove()  # Remove the legend from the left subplot\n",
    "\n",
    "# Plot for Protein\n",
    "ax = sns.histplot(adata_2_prot.obs, x='archetype_label', hue='major_cell_types', multiple='fill', stat='proportion',\n",
    "                  ax=axes[1])\n",
    "axes[1].set_xticks(axes[1].get_xticks())  # Set the ticks first\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_title('Proportion of Archetypes in Major Cell Types in Protein')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# TODO this plot does not add up for some reason, it seems that the archetypes are not aligned correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_points_rna_plot),len(ordered_best_rna_archetype),len(samples_cell_types_rna_plot),len(data_point_archetype_indices_rna_plot),\n",
    "data_points_prot_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    data_points_rna = adata_1_rna.obsm['X_pca']\n",
    "    data_points_prot = adata_2_prot.obsm['X_pca']\n",
    "    data_point_archetype_indices_rna = list(np.argmax(cells_archetype_vec_rna, axis=1))\n",
    "    data_point_archetype_indices_prot = list(np.argmax(cells_archetype_vec_prot, axis=1))\n",
    "\n",
    "    # use ground truth cell types\n",
    "    show_ground_truth = True\n",
    "    if show_ground_truth:\n",
    "        major_or_minor = 'cell_types'\n",
    "        major_or_minor = 'major_cell_types'\n",
    "        samples_cell_types_rna = list(adata_1_rna.obs[major_or_minor])\n",
    "        samples_cell_types_prot = list(adata_2_prot.obs[major_or_minor])\n",
    "    else:  #\n",
    "        # Get the archetype indices for each data point\n",
    "        major_cell_types_list = minor_cell_types_list_rna\n",
    "        current_cell_types_list = major_cell_types_list\n",
    "        # Map the archetype indices to cell type names\n",
    "        samples_cell_types_rna = [current_cell_types_list[i] for i in data_point_archetype_indices_rna]\n",
    "        samples_cell_types_prot = [current_cell_types_list[i] for i in data_point_archetype_indices_prot]\n",
    "\n",
    "    # Optionally limit the number of samples\n",
    "    num_samples = 50000  # or any number you prefer\n",
    "    data_points_rna_plot = data_points_rna[:num_samples]\n",
    "    data_points_prot_plot = data_points_prot[:num_samples]\n",
    "    samples_cell_types_rna_plot = samples_cell_types_rna[:num_samples]\n",
    "    samples_cell_types_prot_plot = samples_cell_types_prot[:num_samples]\n",
    "    data_point_archetype_indices_rna_plot = data_point_archetype_indices_rna[:num_samples]\n",
    "    data_point_archetype_indices_prot_plot = data_point_archetype_indices_prot[:num_samples]\n",
    "\n",
    "    # Create a consistent color mapping\n",
    "    all_cell_types = set(samples_cell_types_rna_plot + samples_cell_types_prot_plot)\n",
    "    all_cell_types.discard('archetype')\n",
    "    all_cell_types = [ct for ct in all_cell_types if ct is not np.nan]\n",
    "    all_cell_types = sorted(all_cell_types)\n",
    "    palette = sns.color_palette(\"tab20\", len(all_cell_types))\n",
    "    cell_type_colors = {cell_type: color for cell_type, color in zip(all_cell_types, palette)}\n",
    "    cell_type_colors[\"archetype\"] = \"black\"\n",
    "\n",
    "    # Call the updated function with the color mapping\n",
    "    plot_archetypes(\n",
    "        data_points_rna_plot,\n",
    "        ordered_best_rna_archetype,\n",
    "        samples_cell_types_rna_plot,\n",
    "        data_point_archetype_indices_rna_plot,\n",
    "        modality='RNA',\n",
    "        cell_type_colors=cell_type_colors\n",
    "    )\n",
    "    plot_archetypes(\n",
    "        data_points_prot_plot,\n",
    "        ordered_best_protein_archetype,\n",
    "        samples_cell_types_prot_plot,\n",
    "        data_point_archetype_indices_prot_plot,\n",
    "        modality='Protein',\n",
    "        cell_type_colors=cell_type_colors\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if plot_flag:\n",
    "    sc.pp.pca(adata_archetype_rna)\n",
    "    sc.pp.pca(adata_archetype_prot)\n",
    "    sc.pl.pca(adata_archetype_rna, color=['major_cell_types', 'archetype_label', 'cell_types'])\n",
    "    sc.pl.pca(adata_archetype_prot, color=['major_cell_types', 'archetype_label', 'cell_types'])\n",
    "    sc.pp.neighbors(adata_archetype_rna)\n",
    "    sc.pp.neighbors(adata_archetype_prot)\n",
    "    sc.tl.umap(adata_archetype_rna)\n",
    "    sc.tl.umap(adata_archetype_prot)\n",
    "    sc.pl.umap(adata_archetype_rna, color=['major_cell_types', 'archetype_label', 'cell_types'])\n",
    "    sc.pl.umap(adata_archetype_prot, color=['major_cell_types', 'archetype_label', 'cell_types'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_flag:\n",
    "    # making sure that the archetypes make sense in original data context\n",
    "    sc.pp.neighbors(adata_1_rna)\n",
    "    sc.pp.neighbors(adata_2_prot)\n",
    "    sc.tl.umap(adata_1_rna)\n",
    "    sc.tl.umap(adata_2_prot)\n",
    "    sc.pl.umap(adata_1_rna, color='archetype_label', title='RNA Archetypes')\n",
    "    sc.pl.umap(adata_2_prot, color='archetype_label', title='Protein Archetypes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_1_rna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_terminal_exhaustion = [\n",
    "    \"CD3G\", \"FASLG\", \"ID2\", \"LAG3\", \"RGS1\", \"CCL3\", \"CCL3L1\", \"KIAA1671\",\n",
    "    \"SH2D2A\", \"DUSP2\", \"PDCD1\", \"CD7\", \"NR4A2\", \"CD160\", \"PTPN22\", \"ABI3\",\n",
    "    \"PTGER4\", \"GZMK\", \"GZMA\", \"MBNL1\", \"VMP1\", \"PLAC8\", \"RGS3\", \"EFHD2\",\n",
    "    \"GLRX\", \"CXCR6\", \"ARL6IP1\", \"CCL4\", \"ISG15\", \"LAX1\", \"CD8A\", \"SERPINA3\",\n",
    "    \"GZMB\", \"TOX\"\n",
    "]\n",
    "\n",
    "t_cell_precursor_exhaustion = [\n",
    "    \"TCF7\", \"MS4A4A\", \"TNFSF8\", \"CXCL10\", \"EEF1B2\", \"ID3\", \"IL7R\", \"JUN\",\n",
    "    \"LTB\", \"XCL1\", \"SOCS3\", \"TRAF1\", \"EMB\", \"CRTAM\", \"EEF1G\", \"CD9\",\n",
    "    \"ITGB1\", \"GPR183\", \"ZFP36L1\", \"SLAMF6\", \"LY6E\"\n",
    "]\n",
    "\n",
    "t_cell_t_reg = [\n",
    "    \"NT5E\", \"CD3D\", \"CD3G\", \"CD3E\", \"CD4\",\n",
    "    \"CD5\", \"ENTPD1\", \"CTLA4\", \"IZUMO1R\", \"TNFRSF18\",\n",
    "    \"IL2RA\", \"ITGAE\", \"LAG3\", \"TGFB1\", \"LRRC32\",\n",
    "    \"TNFRSF4\", \"SELL\", \"FOXP3\", \"STAT5A\", \"STAT5B\",\n",
    "    \"LGALS1\", \"IL10\", \"IL12A\", \"EBI3\", \"TGFB1\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
